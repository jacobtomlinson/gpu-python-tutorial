{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65312eb9-ddbc-4c05-9857-1ac431bd8e66",
   "metadata": {},
   "source": [
    "# CUDA Array Interface\n",
    "\n",
    "Because moving data from the CPU to GPU is expensive we want to keep as much data located on the GPU as possible at all times.\n",
    "\n",
    "Sometimes in our workflow we want to change which tool we are using too. Perhaps we load an array of data with `cupy` but we want to write a custom CUDA kernel with `numba`. Or perhaps we want to switch to using a Deep Learning framework like `pytorch`. \n",
    "\n",
    "When any of these libraries load data onto the GPU the array in memory is pretty much the same, the differences between a cupy `ndarray` and a numba `DeviceNDArray` just boil down to how that array is wrapped and hooked into Python.\n",
    "\n",
    "Thankfully with utilities like [DLPack](https://github.com/dmlc/dlpack) and [__ cuda_array__interface __](https://numba.pydata.org/numba-doc/dev/cuda/cuda_array_interface.html) we can convert from one type to another without modifying the data on the GPU. We just create a new Python wrapper object and transfer all the device pointers accross.\n",
    "\n",
    "Ensuring compatibility between popular GPU Python libraries is one of the core goals of the RAPIDS community.\n",
    "\n",
    "![](images/array-interface.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a072fd72-44bd-4371-95b6-e853da8d7cb6",
   "metadata": {},
   "source": [
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e991c2ac-4cbd-4e3c-9b1e-22f8338876dc",
   "metadata": {},
   "source": [
    "We start off my creating an array with cupy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9dbc32-409c-45bd-8d4d-a5662030ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "cp_arr = cp.random.random((1, 100_000, 10_000))\n",
    "cp_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f132109-0042-417e-8b21-7ad73cc3aa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cp_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2de53d-0d7f-4b3b-9859-f08520ed8bb7",
   "metadata": {},
   "source": [
    "Now let's convert this to a Numba array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191859ce-56c1-480f-ad6c-d76051c08c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "numba_arr = cuda.to_device(cp_arr)\n",
    "numba_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18613c12-d419-4bc5-9bf0-15c07818e536",
   "metadata": {},
   "source": [
    "_Notice that the GPU memory usage stays the same. This is because both `cp_arr` and `numba_arr` reference the same underlying data array, but are different types._\n",
    "\n",
    "We can also convert our array to a pytorch `Tensor` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dd98ce-1ff2-4fde-bd62-865d6a042a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Requires pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d891c3-7499-4a37-a064-bf6f73c174ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_arr = torch.as_tensor(numba_arr, device='cuda')\n",
    "torch_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc1495d-04c1-4746-9242-a05795586ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(torch_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
